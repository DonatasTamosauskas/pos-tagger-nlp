{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build and train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "def train_model(train_file, model_file):\n",
    "    # write your code here. You can add functions as well.\n",
    "    # use torch library to save model parameters, hyperparameters, etc. to model_file\n",
    "    print('Finished...')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # make no changes here\n",
    "    train_file = sys.argv[1]\n",
    "    model_file = sys.argv[2]\n",
    "    train_model(train_file, model_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and do the inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import sys\n",
    "import torch\n",
    "\n",
    "def tag_sentence(test_file, model_file, out_file):\n",
    "    # write your code here. You can add functions as well.\n",
    "\t\t# use torch library to load model_file\n",
    "    print('Finished...')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # make no changes here\n",
    "    test_file = sys.argv[1]\n",
    "    model_file = sys.argv[2]\n",
    "    out_file = sys.argv[3]\n",
    "    tag_sentence(test_file, model_file, out_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The network:\n",
    "1. CNN character level word embedder\n",
    "1. concatenate CNN embedding with word embedding\n",
    "1. bi-directional LSTM block, looking at a sentence\n",
    "1. fully conncected layer? (what does linear projection mean?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fastprogress\n",
      "  Downloading https://files.pythonhosted.org/packages/83/db/794db47024a26c75635c35f0ee5431aa8b528e895ad1ed958041290f83f7/fastprogress-0.1.21-py3-none-any.whl\n",
      "Installing collected packages: fastprogress\n",
      "Successfully installed fastprogress-0.1.21\n",
      "\u001b[33mYou are using pip version 9.0.3, however version 19.3.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install fastprogress\n",
    "from fastprogress import progress_bar\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "training_data = Path(\"../data/sents.train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating data input pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data\n",
    "\n",
    "\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, path, to_lower=True):\n",
    "        self.to_lower = to_lower\n",
    "        self.sentences = []\n",
    "        self.vocab = []\n",
    "        self.tags = []\n",
    "        \n",
    "        self.generate_dataset(path)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        sentence_embs, tag_embs = self.transform_sentence(self.sentences[index])\n",
    "        return sentence_embs, tag_embs\n",
    "    \n",
    "    def generate_dataset(self, path):\n",
    "        with open(path, 'r') as input_file:\n",
    "            self.sentences = input_file.read().split(\"\\n\")\n",
    "            self.create_vocabs(self.sentences)\n",
    "            \n",
    "            self.vocab_size = len(self.vocab)\n",
    "            self.tag_size = len(self.tags)\n",
    "            \n",
    "            if self.sentences[-1] == \"\":\n",
    "                self.sentences.pop()\n",
    "            \n",
    "    \n",
    "    def create_vocabs(self, sentences):\n",
    "        vocab_set = set()\n",
    "        tag_set = set()\n",
    "\n",
    "        for sentence in sentences:\n",
    "            for word in sentence.split(\" \"):\n",
    "                try:\n",
    "                    word, tag = self.split_words_tag(word)\n",
    "                    vocab_set.add(word.lower() if self.to_lower else word)\n",
    "                    tag_set.add(tag)\n",
    "                except RuntimeError:\n",
    "                    print(\"Not a valid word/tag pair: \" + word)\n",
    "\n",
    "        self.vocab = list(vocab_set)\n",
    "        self.tags = list(tag_set)\n",
    "            \n",
    "    def transform_sentence(self, sentence):\n",
    "        numeric_sent = []\n",
    "        tags = []\n",
    "\n",
    "        for word_tag in sentence.split(\" \"):\n",
    "            try:\n",
    "                word, tag = self.split_words_tag(word_tag)\n",
    "                tag_id = self.tags.index(tag)\n",
    "                word_id = self.vocab.index(word.lower() if self.to_lower else word)\n",
    "\n",
    "            except RuntimeError:\n",
    "                print(\"Not a valid word/tag pair: \" + word_tag)\n",
    "            except ValueError:\n",
    "                print(\"Word not in the vocab: \" + word_tag)\n",
    "                # The id of an unknown word\n",
    "                word_id = len(self.vocab)\n",
    "                tag_id = len(self.tags)\n",
    "\n",
    "            numeric_sent.append(word_id)\n",
    "            tags.append(tag_id)\n",
    "\n",
    "        return torch.tensor(numeric_sent), torch.tensor(tags)\n",
    "\n",
    "    @staticmethod\n",
    "    def split_words_tag(word):\n",
    "        words_tag = word.split(\"/\")\n",
    "        \n",
    "        if len(words_tag) < 2: \n",
    "            raise RuntimeError(\"Not a valid word/tag pair:\" + word)\n",
    "            \n",
    "        tag = words_tag.pop()\n",
    "        word = \"/\".join(words_tag)\n",
    "        \n",
    "        return word, tag\n",
    "                \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not a valid word/tag pair: \n",
      "CPU times: user 1.56 s, sys: 28.1 ms, total: 1.59 s\n",
      "Wall time: 1.58 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dataset = Dataset(training_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataLoader implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 1\n",
    "num_workers = 4\n",
    "\n",
    "\n",
    "pos_dataloader = DataLoader(dataset, batch_size=batch_size, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial simple model implementation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proposed plan:\n",
    "1. Begin with word-level LSTM (check for an example in forum)\n",
    "2. Make it bi-directional\n",
    "3. Add character-level CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try a new notebook: https://polynote.org/docs/01-installation.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PipelineTestModel(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dims, output_dims):\n",
    "        super(PipelineTestModel, self).__init__()\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        self.emb_dims = emb_dims\n",
    "        self.output_dims = output_dims\n",
    "        \n",
    "        self.emb = nn.Embedding(self.vocab_size, self.emb_dims)\n",
    "        self.fc = nn.LSTM(self.emb_dims, self.output_dims)\n",
    "        \n",
    "    def forward(self, sentence):\n",
    "        print(sentence.shape)\n",
    "        emb = self.emb(sentence)\n",
    "        print(emb.shape)\n",
    "#         tags = F.softmax(self.fc(emb), dim=self.output_dims)\n",
    "        tags = self.fc(emb.view(len(sentence), 1, -1))\n",
    "        print(tags.shape)\n",
    "        return tags    \n",
    "    \n",
    "    \n",
    "class LSTMTagger(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n",
    "        super(LSTMTagger, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
    "        # with dimensionality hidden_dim.\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
    "\n",
    "        # The linear layer that maps from hidden state space to tag space\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        lstm_out, _ = self.lstm(embeds.view(len(sentence), 1, -1))\n",
    "        tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))\n",
    "        tag_scores = F.log_softmax(tag_space, dim=1)\n",
    "        return tag_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dims = 128\n",
    "hidden_dims = 64\n",
    "vocab_size = dataset.vocab_size\n",
    "tagset_size = dataset.tag_size\n",
    "\n",
    "model = LSTMTagger(embedding_dims, hidden_dims, vocab_size, tagset_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_dataloader = DataLoader(dataset, batch_size=batch_size, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Cannot initialize CUDA without ATen_cuda library. PyTorch splits its backend into two shared libraries: a CPU library and a CUDA library; this error has occurred because you are trying to use some CUDA functionality, but the CUDA library has not been loaded by the dynamic linker for some reason.  The CUDA library MUST be loaded, EVEN IF you don't directly use any symbols from the CUDA library! One common culprit is a lack of -Wl,--no-as-needed in your link arguments; many dynamic linkers will delete dynamic library dependencies if you don't depend on any of their symbols.  You can check if this has occurred by using ldd on your binary to see if there is a dependency on *_cuda.so library.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-245-129da4c04090>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpos_dataloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mx_gpu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_gpu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgpu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgpu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: Cannot initialize CUDA without ATen_cuda library. PyTorch splits its backend into two shared libraries: a CPU library and a CUDA library; this error has occurred because you are trying to use some CUDA functionality, but the CUDA library has not been loaded by the dynamic linker for some reason.  The CUDA library MUST be loaded, EVEN IF you don't directly use any symbols from the CUDA library! One common culprit is a lack of -Wl,--no-as-needed in your link arguments; many dynamic linkers will delete dynamic library dependencies if you don't depend on any of their symbols.  You can check if this has occurred by using ldd on your binary to see if there is a dependency on *_cuda.so library."
     ]
    }
   ],
   "source": [
    "gpu = torch.device(\"cuda\")\n",
    "\n",
    "for epoch in range(1):\n",
    "    for x, y in pos_dataloader:\n",
    "        x_gpu, y_gpu = x.to(gpu), y.to(gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
