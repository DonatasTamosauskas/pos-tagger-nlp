{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Build and train the model"},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nimport math\nimport sys\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\ndef train_model(train_file, model_file):\n    # write your code here. You can add functions as well.\n    # use torch library to save model parameters, hyperparameters, etc. to model_file\n    print('Finished...')\n\nif __name__ == \"__main__\":\n    # make no changes here\n    train_file = sys.argv[1]\n    model_file = sys.argv[2]\n    train_model(train_file, model_file)\n","execution_count":1,"outputs":[{"output_type":"stream","text":"Finished...\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"# Load and do the inference"},{"metadata":{},"cell_type":"markdown","source":"# The network:\n1. CNN character level word embedder\n1. concatenate CNN embedding with word embedding\n1. bi-directional LSTM block, looking at a sentence\n1. fully conncected layer? (what does linear projection mean?)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# !pip install fastprogress matplotlib\nfrom fastprogress import progress_bar, master_bar\nimport matplotlib.pyplot as plt\nfrom pathlib import Path\nimport numpy as np\n\n,KAGGLE = True\n\ntraining_data = Path(\"../input/sents.train\") if KAGGLE else Path(\"../data/sents.train\")","execution_count":2,"outputs":[{"output_type":"stream","text":"Requirement already satisfied: fastprogress in /opt/conda/lib/python3.6/site-packages (0.1.21)\r\nRequirement already satisfied: matplotlib in /opt/conda/lib/python3.6/site-packages (3.0.3)\r\nRequirement already satisfied: numpy>=1.10.0 in /opt/conda/lib/python3.6/site-packages (from matplotlib) (1.16.4)\r\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.6/site-packages (from matplotlib) (0.10.0)\r\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.6/site-packages (from matplotlib) (1.1.0)\r\nRequirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /opt/conda/lib/python3.6/site-packages (from matplotlib) (2.4.2)\r\nRequirement already satisfied: python-dateutil>=2.1 in /opt/conda/lib/python3.6/site-packages (from matplotlib) (2.8.0)\r\nRequirement already satisfied: six in /opt/conda/lib/python3.6/site-packages (from cycler>=0.10->matplotlib) (1.12.0)\r\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.6/site-packages (from kiwisolver>=1.0.1->matplotlib) (41.4.0)\r\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"# Creating data input pipeline"},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\nimport torch.utils.data\n\n\nclass Dataset(torch.utils.data.Dataset):\n    def __init__(self, path, to_lower=True, training=True):\n        self.to_lower = to_lower\n        self.training = training\n        \n        self.sentences = []\n        self.vocab = []\n        self.tags = []\n        \n        self.generate_dataset(path)\n    \n    def __len__(self):\n        return len(self.sentences)\n    \n    def __getitem__(self, index):\n        sentence_embs, tag_embs = self.transform_sentence(self.sentences[index])\n        return sentence_embs, tag_embs\n    \n    def generate_dataset(self, path):\n        with open(path, 'r') as input_file:\n            self.sentences = input_file.read().split(\"\\n\")\n            \n            if len(self.vocab) == 0:\n                self.create_vocabs(self.sentences)\n                self.vocab_size = len(self.vocab)\n                self.tag_size = len(self.tags)\n            \n            if self.sentences[-1] == \"\":\n                self.sentences.pop()\n    \n    def create_vocabs(self, sentences):\n        vocab_set = set()\n        tag_set = set()\n\n        for sentence in sentences:\n            for word in sentence.split(\" \"):\n                try:\n                    word, tag = self.split_words_tag(word)\n                    vocab_set.add(word.lower() if self.to_lower else word)\n                    tag_set.add(tag)\n                except RuntimeError:\n                    print(\"Not a valid word/tag pair: \" + word)\n\n        self.vocab = list(vocab_set)\n        self.tags = list(tag_set)\n            \n    def transform_sentence(self, sentence):\n        numeric_sent = []\n        tags = []\n\n        for word_tag in sentence.split(\" \"):\n            try:\n                if self.training:\n                    word, tag = self.split_words_tag(word_tag)\n                    tag_id = self.tags.index(tag)\n                else:\n                    word = word_tag\n                    \n                word_id = self.vocab.index(word.lower() if self.to_lower else word)\n\n            except RuntimeError:\n                print(\"Not a valid word/tag pair: \" + word_tag)\n            except ValueError:\n                print(\"Word not in the vocab: \" + word_tag)\n                # The id of an unknown word\n                word_id = len(self.vocab) - 1\n\n            numeric_sent.append(word_id)\n            if self.training: tags.append(tag_id)\n\n        return torch.tensor(numeric_sent), torch.tensor(tags) if self.training else []\n\n    @staticmethod\n    def split_words_tag(word):\n        words_tag = word.split(\"/\")\n        \n        if len(words_tag) < 2: \n            raise RuntimeError(\"Not a valid word/tag pair:\" + word)\n            \n        tag = words_tag.pop()\n        word = \"/\".join(words_tag)\n        \n        return word, tag\n    \n    def print_sentence(self, sentence):\n        print(\" \".join([self.vocab[word.item()] for word in sentence.view(-1)]))\n                \n    def decode_sentence(self, sentence):\n        return [self.vocab[word.item()] for word in sentence.view(-1)]\n    \n    def decode_tags(self, tag_ids):\n        return [self.tags[tag.item()] for tag in tag_ids.view(-1)]\n    \n    def __getstate__(self):\n        d = dict(self.__dict__)\n        del d['sentences']\n        return d\n    \n    def __setstate(self, d):\n        self.__dict__.update(d)\n        self.__dict__.update({'sentences': []})","execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ndataset = Dataset(training_data)","execution_count":4,"outputs":[{"output_type":"stream","text":"Not a valid word/tag pair: \nCPU times: user 2 s, sys: 8 ms, total: 2 s\nWall time: 2.02 s\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## DataLoader implementation"},{"metadata":{"trusted":true},"cell_type":"code","source":"from torch.utils.data import DataLoader\nfrom torch.nn.utils.rnn import pad_sequence\n\nbatch_size = 128\nnum_workers = 4\n\ndef pad_seq(sequences):\n    x, y = [], []\n    \n    for embs, targets in sequences:\n        x.append(embs)\n        y.append(targets)\n        \n    return pad_sequence(x, batch_first=True), pad_sequence(y, batch_first=True)\n\npos_dataloader = DataLoader(dataset, batch_size=1, num_workers=num_workers)\npos_dataloader_batched = DataLoader(dataset, batch_size=batch_size, collate_fn=pad_seq, num_workers=num_workers)","execution_count":9,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Initial simple model implementation "},{"metadata":{},"cell_type":"markdown","source":"## Proposed plan:\n1. Begin with word-level LSTM (check for an example in forum)\n2. Make it bi-directional\n3. Add character-level CNN"},{"metadata":{},"cell_type":"markdown","source":"Try a new notebook: https://polynote.org/docs/01-installation.html"},{"metadata":{"trusted":true},"cell_type":"code","source":"class PipelineTestModel(nn.Module):\n    def __init__(self, vocab_size, emb_dims, output_dims):\n        super(PipelineTestModel, self).__init__()\n        \n        self.vocab_size = vocab_size\n        self.emb_dims = emb_dims\n        self.output_dims = output_dims\n        \n        self.emb = nn.Embedding(self.vocab_size, self.emb_dims)\n        self.fc = nn.LSTM(self.emb_dims, self.output_dims)\n        \n    def forward(self, sentence):\n        print(sentence.shape)\n        emb = self.emb(sentence)\n        print(emb.shape)\n#         tags = F.softmax(self.fc(emb), dim=self.output_dims)\n        tags = self.fc(emb.view(len(sentence), 1, -1))\n        print(tags.shape)\n        return tags    \n    \n    \nclass LSTMTagger(nn.Module):\n\n    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n        super(LSTMTagger, self).__init__()\n        self.hidden_dim = hidden_dim\n\n        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n\n        # The LSTM takes word embeddings as inputs, and outputs hidden states\n        # with dimensionality hidden_dim.\n        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n\n        # The linear layer that maps from hidden state space to tag space\n        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n    \n    def forward(self, sentence):\n        batches = sentence.shape[0]\n        embeds = self.word_embeddings(sentence)\n#         print(embeds.shape)\n#         print(embeds.view(len(sentence[-1]), batches, -1).shape)\n        lstm_out, _ = self.lstm(embeds.view(len(sentence[-1]), batches, -1))\n        tag_space = self.hidden2tag(lstm_out.view(len(sentence[-1]) * batches, -1))\n        tag_scores = F.log_softmax(tag_space, dim=1)\n        return tag_scores","execution_count":6,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding_dims = 128\nhidden_dims = 64\nvocab_size = dataset.vocab_size\ntagset_size = dataset.tag_size\n\nmodel = LSTMTagger(embedding_dims, hidden_dims, vocab_size, tagset_size)","execution_count":7,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\ndevice = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\nprint(\"Device: {}\". format(device))\n\nembedding_dims = 128\nhidden_dims = 128\nvocab_size = dataset.vocab_size + 1 # For unkown words\ntagset_size = dataset.tag_size\n\nmodel = LSTMTagger(embedding_dims, hidden_dims, vocab_size, tagset_size)\nloss_func = nn.NLLLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.01)\n\nepochs = 1\nfake_batch = 32\n\ntag_size = dataset.tag_size\nlosses = []\n\ni = 0\n\nmodel.to(device)\nmodel.zero_grad()\n\nmaster = master_bar(range(epochs))\nfor epoch in master:\n    for x, y in progress_bar(pos_dataloader_batched, parent=master):\n            \n        model.zero_grad()\n        x, y = x.to(device), y.to(device)\n#         print(x)\n#         print(x.shape)\n#         print(\"\\n\")\n        \n#         print(y)\n#         print(y.shape)\n#         print(\"\\n\")\n        \n        pred = model(x)\n        loss = loss_func(pred.view(-1, tag_size), y.view(-1))\n        loss = loss_func(pred.view(-1, tag_size), y.view(-1))\n\n        loss.backward()\n        optimizer.step()\n                    \n        if i % 10 == 0: \n            losses.append(loss.item())\n            !nvidia-smi\n        i += 1\n        \n        \n# for epoch in master:\n#     for x, y in progress_bar(pos_dataloader, parent=master):\n#         if i % fake_batch == 0:\n#             optimizer.step() # does not work\n#             model.zero_grad()\n\n#         x, y = x.to(gpu), y.to(gpu)\n#         pred = model(x)\n#         loss = loss_func(pred.view(-1, tag_size), y.view(-1))\n#         loss.backward()\n            \n#         if i % 100 == 0: losses.append(loss.item())\n#         i += 1\n        ","execution_count":12,"outputs":[{"output_type":"stream","text":"Device: cuda\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n        <style>\n            /* Turns off some styling */\n            progress {\n                /* gets rid of default border in Firefox and Opera. */\n                border: none;\n                /* Needs to be in here for Safari polyfill so background images work as expected. */\n                background-size: auto;\n            }\n            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n                background: #F44336;\n            }\n        </style>\n      <progress value='0' class='' max='1', style='width:300px; height:20px; vertical-align: middle;'></progress>\n      0.00% [0/1 00:00<00:00]\n    </div>\n    \n\n\n    <div>\n        <style>\n            /* Turns off some styling */\n            progress {\n                /* gets rid of default border in Firefox and Opera. */\n                border: none;\n                /* Needs to be in here for Safari polyfill so background images work as expected. */\n                background-size: auto;\n            }\n            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n                background: #F44336;\n            }\n        </style>\n      <progress value='0' class='progress-bar-interrupted' max='312', style='width:300px; height:20px; vertical-align: middle;'></progress>\n      Interrupted\n    </div>\n    "},"metadata":{}},{"output_type":"stream","text":"Tue Nov  5 15:06:05 2019       \n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 418.67       Driver Version: 418.67       CUDA Version: 10.1     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|===============================+======================+======================|\n|   0  Tesla P100-PCIE...  On   | 00000000:00:04.0 Off |                    0 |\n| N/A   48C    P0    36W / 250W |    919MiB / 16280MiB |      0%      Default |\n+-------------------------------+----------------------+----------------------+\n                                                                               \n+-----------------------------------------------------------------------------+\n| Processes:                                                       GPU Memory |\n|  GPU       PID   Type   Process name                             Usage      |\n|=============================================================================|\n+-----------------------------------------------------------------------------+\nTue Nov  5 15:06:18 2019       \n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 418.67       Driver Version: 418.67       CUDA Version: 10.1     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|===============================+======================+======================|\n|   0  Tesla P100-PCIE...  On   | 00000000:00:04.0 Off |                    0 |\n| N/A   48C    P0    35W / 250W |    919MiB / 16280MiB |      0%      Default |\n+-------------------------------+----------------------+----------------------+\n                                                                               \n+-----------------------------------------------------------------------------+\n| Processes:                                                       GPU Memory |\n|  GPU       PID   Type   Process name                             Usage      |\n|=============================================================================|\n+-----------------------------------------------------------------------------+\nTue Nov  5 15:06:34 2019       \n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 418.67       Driver Version: 418.67       CUDA Version: 10.1     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|===============================+======================+======================|\n|   0  Tesla P100-PCIE...  On   | 00000000:00:04.0 Off |                    0 |\n| N/A   48C    P0    36W / 250W |    919MiB / 16280MiB |      0%      Default |\n+-------------------------------+----------------------+----------------------+\n                                                                               \n+-----------------------------------------------------------------------------+\n| Processes:                                                       GPU Memory |\n|  GPU       PID   Type   Process name                             Usage      |\n|=============================================================================|\n+-----------------------------------------------------------------------------+\nTue Nov  5 15:06:46 2019       \n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 418.67       Driver Version: 418.67       CUDA Version: 10.1     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|===============================+======================+======================|\n|   0  Tesla P100-PCIE...  On   | 00000000:00:04.0 Off |                    0 |\n| N/A   48C    P0    36W / 250W |    919MiB / 16280MiB |      0%      Default |\n+-------------------------------+----------------------+----------------------+\n                                                                               \n+-----------------------------------------------------------------------------+\n| Processes:                                                       GPU Memory |\n|  GPU       PID   Type   Process name                             Usage      |\n|=============================================================================|\n+-----------------------------------------------------------------------------+\nTue Nov  5 15:07:03 2019       \n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 418.67       Driver Version: 418.67       CUDA Version: 10.1     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|===============================+======================+======================|\n|   0  Tesla P100-PCIE...  On   | 00000000:00:04.0 Off |                    0 |\n| N/A   49C    P0    36W / 250W |    919MiB / 16280MiB |      0%      Default |\n+-------------------------------+----------------------+----------------------+\n                                                                               \n+-----------------------------------------------------------------------------+\n| Processes:                                                       GPU Memory |\n|  GPU       PID   Type   Process name                             Usage      |\n|=============================================================================|\n+-----------------------------------------------------------------------------+\n","name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/fastprogress/fastprogress.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    802\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    803\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 804\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    805\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    770\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 771\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    772\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    773\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    722\u001b[0m         \u001b[0;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    723\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 724\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    725\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/multiprocessing/queues.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    102\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m                     \u001b[0mtimeout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeadline\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/multiprocessing/connection.py\u001b[0m in \u001b[0;36mpoll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_readable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    412\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 414\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    415\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/multiprocessing/connection.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    909\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    910\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 911\u001b[0;31m                 \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    912\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    913\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfileobj\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevents\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/selectors.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    374\u001b[0m             \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 376\u001b[0;31m                 \u001b[0mfd_event_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    377\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInterruptedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"metadata":{},"cell_type":"markdown","source":"## Pad the input so that it is possible to use mini-batches\n\n- Padding | https://discuss.pytorch.org/t/understanding-pack-padded-sequence-and-pad-packed-sequence/4099\n- Padding | https://discuss.pytorch.org/t/simple-working-example-how-to-use-packing-for-variable-length-sequence-inputs-for-rnn/2120\n\n## The padding needs to take place in the DataLoader\n- Most likely will need to use sampler & collate_fn\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(losses)\nprint(min(losses))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Exporting trained model and built dictionary"},{"metadata":{"trusted":false},"cell_type":"code","source":"def export_model(model, dataset, train_time):\n    model_name = str(type(model)).split(\".\")[-1][:-2]\n    model_save_name = model_name + \"_\" + str(train_time)\n    \n    torch.save(model, model_save_name + \"_1.data\")\n    torch.save(dataset, model_save_name + \"_2.data\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"export_model(model, dataset, 5)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"markdown","source":"# The inference notebook"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data = Path(\"../input/sents.test\")\ndataset.generate_dataset(test_data)\ndataset.training = False","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nfrom torch.utils.data import DataLoader\n\ngpu = torch.device(\"cuda\")\ncpu = torch.device(\"cpu\")\nmodel.to(cpu)\ndataloader = DataLoader(dataset, batch_size=1, shuffle=False, num_workers=4)\n\ni = 0\n\npreds = []\nfor x, _ in dataloader:\n#     if i > 100:\n#         break\n#     i += 1\n    predictions = model(x.to(cpu))\n    _, pos_tag_ids = predictions.max(1)\n    \n    words = dataset.decode_sentence(x)\n    tags = dataset.decode_tags(pos_tag_ids)\n    word_tags = [\"/\".join(word_tag) for word_tag in zip(words, tags)]\n    \n    preds.append(\" \".join(word_tags))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def export_preds(preds, filename):\n    with open(filename, \"w\") as out_file:\n        out_file.write(\"\\n\".join(preds))\n        \nexport_preds(preds, \"test_output.txt\")\n\n!python3 ../input/eval.py test_output.txt ../input/sents.answer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.9"}},"nbformat":4,"nbformat_minor":1}